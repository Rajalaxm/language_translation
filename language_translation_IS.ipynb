{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_translation_IS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMBQV0WRjDuzIjaPUQDbB1l",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rajalaxm/language_translation/blob/main/language_translation_IS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TV85ZuFamgHw"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "pd.set_option('display.max_colwidth',200)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMbLm6C6roak"
      },
      "source": [
        "Our data is a text file of English-German sentence pairs.First we will read the file using the function defined below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ScY8b5dGr_6s"
      },
      "source": [
        "#function to read raw text file\n",
        "def read_text(filename):\n",
        "  #open the file\n",
        "  file= open(filename, mode='rt', encoding= 'utf-8')\n",
        "  #read all text\n",
        "  text= file.read()\n",
        "  file.close()\n",
        "  return text"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDqtFewws8r6"
      },
      "source": [
        "Now let's define a function to split the text into English-German pairs separated by '\\n' and then split these pairs into English sentences and German sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQXyAo-ltamh"
      },
      "source": [
        "#split a text into sentences\n",
        "def to_lines(text):\n",
        "  sents= text.strip().split('\\n')\n",
        "  sents= [i.split('\\t') for i in sents]\n",
        "  return sents"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54eIV4AmumPX",
        "outputId": "4b471418-3396-4838-8e58-e4e5305a4951"
      },
      "source": [
        "data= read_text(\"/content/sample_data/deu.txt\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLvgAWmKxMYZ"
      },
      "source": [
        "The actual data contains over 1,50,000 sentence pairs.However, we will use the first 50,000 sentence pairs only to reduce the training time of the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luwMfnbPxuDo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "476ec6a4-ceef-45ca-e449-89b64d03a12f"
      },
      "source": [
        "deu_eng = deu_eng[:50000, :]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cd33398a4210>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdeu_eng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeu_eng\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYSLEm2xxLJG"
      },
      "source": [
        "#Let's take a look at our data\n",
        "deu_eng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4D8NroeMcgM"
      },
      "source": [
        "Text to sequence conversion: \n",
        "To find our data in a Seq2Seq model, we will have to convert both the input and output sentences into integer sequences of fixed length.Before that,let's visualise the length of the sentences.We will capture the length of all the sentences in 2 seperate lists for English and German respectively. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5rrc6JGNm0S"
      },
      "source": [
        "#empty lists\n",
        "eng_1 = []\n",
        "deu_1 = []\n",
        "#populate the lists with sentence lengths\n",
        "for i in deu_eng[ :,0]:\n",
        "  deu_l.append(len(i.split()))\n",
        "\n",
        "  for i in deu_eng[ :,1]:\n",
        "    deu_l.append(len(i.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVIeoCqeOkCY"
      },
      "source": [
        "length_df = pd.DataFrame({'eng': eng_l, 'deu': deu_l})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8na5bOUO11p"
      },
      "source": [
        "length_df.hist(bins =  30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goqX6otDPHM4"
      },
      "source": [
        "The maximum length of the German sentences is 11 and that of the English phrases is 8.\n",
        "Let's vectorize on text data by using Keras's Tokenizer() class.It will turn our sentences into sequences of integers.Then we will pad those sequences with zerosto make all the sequences of same length. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTNQE7ZdQIjb"
      },
      "source": [
        "#function to build a tokenizer\n",
        "def tokenization(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axml5_QOQnln"
      },
      "source": [
        "#prepare English tokenizer \n",
        "eng_tokenizer = tokenization(deu_eng[ :,0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index)+1\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' %eng_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIxYZyCQReLU"
      },
      "source": [
        "#prepare Deutch tokenizer\n",
        "den_tokenizer = tokenization(deu_eng[ :,1])\n",
        "deu_vocab_size= len(den_tokenizer.word_index) +1\n",
        "\n",
        "den_length = 8\n",
        "print('Deutch Vocabulary Size = %d' %deu_vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMX2Z2ZrSe7b"
      },
      "source": [
        "Given below is a function to prepare the sequences.It will also perform sequence padding to a maximum sentence length as mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0r9yOLPuS50V"
      },
      "source": [
        "#encode and pad sequences\n",
        "def encode_sequences(tokenizer,length,lines):\n",
        "  #integer encode sequences\n",
        "  seq = tokenizer.texts_to_sequences(lines)\n",
        "  #pad sequences with 0 values\n",
        "  seq = pad_sequences(seq, maxlen= length,padding = 'post')\n",
        "  return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAtc3ruyTrUp"
      },
      "source": [
        "Model building\n",
        "We will now split the data into train and test set for modeltraining and evaluation respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKfTZF9BUDj5"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(deu_eng, test_size = 0.2, random_state)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49ac14J7XZmi"
      },
      "source": [
        "It's time to encode the sentences.We will encode German sentences as the input sequences and English sentences as the target sequences.It will be done for both train and test data sets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "170e1oBNX8wD"
      },
      "source": [
        "#prepare training data\n",
        "trainX = encode_sequences(deu_tokenizer, deu_length,train[ :,1])\n",
        "trainY = encode_sequences(eng_tokenizer,eng_length,train[ :,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdLAsDmWbmNh"
      },
      "source": [
        "#prepare validation data\n",
        "trainX = encode_sequences(deu_tokenizer, deu_length,train[ :,1])\n",
        "trainY = encode_sequences(eng_tokenizer,eng_length,train[ :,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS_yOf32cFAG"
      },
      "source": [
        "Now cover the exciting part!Let us define our Seq2Seq model architecture.We are using Embedded layer & an LSTM layer as our encoder and another LSTM layer followed by a Dense layer as the decoder. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78SYGz68eihY"
      },
      "source": [
        "#Build NMT Model\n",
        "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units)\n",
        "     model= sequential\n",
        "     model.add= (Embedding(in_vocab,units, input_length= in_timesteps, mask_zero= True))\n",
        "     model.add(LSTM(units))\n",
        "     model.add(RepealVector(out_timesteps))\n",
        "     model.add(LSTM(units, return_sequences= True))\n",
        "     model.add(Dense(out_vocab, activation='softmax'))\n",
        "     return model\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taQei20KhLEg"
      },
      "source": [
        "We are using RMSprop optimizer in this model as it is usually a good choice for recurrent neural networks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhvwDT4Iiu95"
      },
      "source": [
        "model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "rms= optimizers.RMSprop(lr = 0.001)\n",
        "model.compile = (optimizer = rms, loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yg2X9rqHjgdR"
      },
      "source": [
        "We have used 'sparse_categorical_crossentropy' as the loss function because it allows us to use the target sequence as it is instead of 1 hot encoded format.One hot encoding the target sequences with such a large vocabulary might consume our systems's entire memory.\n",
        "It seems we are all set to start training our model.We will train it for 30 epochs & with a batch size of 512.You may change & play these hyperparameters.We will also be using Modelchekpoint() to save the best model with lowest validation loss.I personally prefer this method over early stopping.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XlAerzpBsSU0"
      },
      "source": [
        "filename = 'model.h1.24_rajalaxmi'\n",
        "checkpoint = ModelCheckpoint(filename, monitor= 'val_loss',verbose=1, save_best_only = True, mode='min')\n",
        "\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0],trainY.shape[1], 1))\n",
        "epochs = 5, batch_size=512,\n",
        "validation_split = 0.2,\n",
        "callbacks = [checkpoint], verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1x9znbjjuAeY"
      },
      "source": [
        "Let's compare the training loss & validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82rw1wr1uGsK"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train', 'validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ne4pbWY3vufl"
      },
      "source": [
        "Let's load the saved model to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ehHC30Mv4js"
      },
      "source": [
        "model = load_model('model.h1.24_rajalaxmi')\n",
        "preds = model.predict_classes(testX.reshape((testX.reshape[0],textX.shape[1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox7wjEEawkN1"
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == n:\n",
        "      return word\n",
        "return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVkPo0SSxxVC"
      },
      "source": [
        "#convert prediction into text(English)\n",
        "preds_text =[]\n",
        "for i in preds:\n",
        "  temp =[]\n",
        "  for j in range(len(i)):\n",
        "    t = get_word(i[j], eng_tokenizer)\n",
        "    if j> 0:\n",
        "      if(t== get_word(i[j], eng_tokenizer))\n",
        "         temp.append('')\n",
        "      else:\n",
        "        temp.append(t)\n",
        "    else:\n",
        "      if(t== None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t)\n",
        "preds_text.append(''.join(temp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmtT8L-1z9Nv"
      },
      "source": [
        "preds_df = pd.DataFrame({'actual': test[ :,0], 'predicted':preds_text})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoOyNfS10ORk"
      },
      "source": [
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24pZhkM30c1p"
      },
      "source": [
        "pred_df.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I-_kAtq0klJ"
      },
      "source": [
        "pred_df.tail(15)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}